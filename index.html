<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Zhongdao Wang</title>
  
  <meta name="author" content="Zhongdao Wang">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<script async defer src="https://buttons.github.io/buttons.js"></script>
<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Zhongdao Wang</name>
              </p>
              <p>I am currently a researcher at Noah's Ark Lab, Huawei. Before joining Huawei, I recieved my Ph.D. degree from the Department of Electronic Engineering, Tsinghua Univerisity, and my bechalor's degree from the Department of Physics, Tsinghua University. During my Ph.D. time, I have been fortunate to work closely with <a href="http://www.liangzheng.com.cn/">Prof. Liang Zheng</a>,
              <a herf="https://yifansun-reid.github.io/">Dr.  Yifan Sun</a>, <a href="https://www.robots.ox.ac.uk/~luca/">Dr. Luca Bertinetto</a>, and <a href="https://hszhao.github.io/">Prof. Hengshuang Zhao</a>
              </p>
              <p style="text-align:center">
                <a href="mailto:wangzhongdao@huawei.com">Email</a> &nbsp/&nbsp
                <a href="data/cv_zhongdaowang.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=VkKYpHIAAAAJ&hl=en-US">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/Zhongdao">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/me.jpeg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/me.jpeg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research Interests</heading>
              <p> 
              <strong>In the long term</strong>, my research interest lies in developing general embodied intelligent agents (an autonomous driving car can be a simplified example). I have a faith that the emergence of general machine intelligence requires observing, interacting with, and learning from the physical world. In the future, embodied agents may be able to <a href="https://tinytraining.mit.edu/">train themselves on the edge</a>, and possibly explore the world <em>in clusters</em>, learning together to form a foundational intelligent model (somewhat like the <a href="https://psychopass.fandom.com/wiki/Sibyl_System">Sibyl system </a> in the Sci-Fi anime <a href="https://en.wikipedia.org/wiki/Psycho-Pass">Psycho-Pass</a>, but an enhanced version that can interact with the world). Such a strong intelligent model may looks a little bit evil, but with efforts I believe we can make it used for good purposes :)
              </p>

              <p>
              <strong>In the short term</strong>, my research interest lies in perception algorithms for automonous driving. Including but not limited to 2D/3D object detection, segmentation, tracking, 3D reconstruction, and offboard pereception (for auto-labeling). <font color="red">I have several openings for self-motivated reserach interns on these topics, please feel free to drop me an e-mail.</font> 
              </p>

              <p>
              <strong>Previously</strong>, I did research on representation learning and object tracking. Representative works include <strong><a href="https://github.com/Zhongdao/Towards-Realtime-MOT">JDE</a></strong> and <strong><a href="https://zhongdao.github.io/UniTrack">UniTrack</a></strong> for object tracking, the <strong><a>CycAs series</a> [<a href="https://arxiv.org/abs/2007.07577">v1</a>,<a href="https://arxiv.org/abs/2211.03663">v2</a>,<a href="">v3</a>]</strong> for person re-identification, and <strong><a href="https://arixv.org/abs/2002.10857">Circle Loss</a>,  <a href=""https://arxiv.org/abs/1908.01281>D-Softmax Loss</a></strong> for metric learning.
              </p>
            </td>
          </tr>
        </tbody></table>
        
        
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Selected Publications</heading>
          <p> 
          For the latest full list please check <a href="https://scholar.google.com/citations?user=VkKYpHIAAAAJ&hl=en">here</a>. 
          </p>
          <br>
          <h3 id="perception"> Perception (object detection/segmentation/tracking) </h3>
            </td>
          </tr>
      </tbody></table>
          <!--
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/miex_.png" style="width:auto; height:auto; max-width:100%;">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2112.01730">
                <papertitle>How to Synthesize a Large-Scale and Trainable Micro-Expression Dataset?</papertitle>
              </a>
              <br>
              Yuchi Liu,
                <strong>Zhongdao Wang</strong>,
              <a href="http://users.cecs.anu.edu.au/~Tom.Gedeon/">Tom Gedeon</a>,
              <a href="http://www.liangzheng.com.cn/">Liang Zheng</a>,
              <br>
              <em>ECCV</em>, 2022
              
              <br>
              <a href="https://arxiv.org/abs/2112.01730">arXiv</a> /
              <a href="https://github.com/liuyvchi/MiE-X">code</a>  <a class="github-button" href="https://github.com/liuyvchi/MiE-X" data-icon="octicon-star" data-show-count="true" aria-label="Star Zhongdao/UniTrack on GitHub" target="_blank">star</a>
              
              <br>
              <p></p>
              <p> A large-scale, synthetic dataset for micro facial expression recognition.</p>
            </td>
          </tr>  
          </tbody></table>

       
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/prob_reid.png" style="width:auto; height:auto; max-width:100%;">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org">
                <papertitle>Reliability-Aware Prediction via Uncertainty Learning for Person Image Retrieval</papertitle>
              </a>
              <br>
              Zhaopeng Dou,
              <strong>Zhongdao Wang</strong>,
              Weihua Chen,
              <a href="https://scholar.google.com/citations?user=aYLh7fsAAAAJ&hl=en&oi=ao">Ya-Li Li</a>,
              <a href="https://scholar.google.com/citations?user=RgzLZZsAAAAJ&hl=en&oi=ao">Shengjin Wang</a>,
              <br>
              <em>ECCV</em>, 2022
              
              <br>
              <p></p>
              <p> Joint modeling of data and model uncertainty improves robustness and reliability of Re-ID representations.</p>
            </td>
          </tr>  
        </tbody></table>
          -->
          
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:30%;vertical-align:middle">
                <img src="images/metabev.png" style="width:auto; height:auto; max-width:100%;">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://chongjiange.github.io/metabev.html">
                <papertitle>MetaBEV: Solving Sensor Failures for BEV Detection and Map Segmentation</papertitle>
              </a>
              <br>
               <a href="https://chongjiange.github.io/">Chongjian Ge</a>,
               <a href="https://scholar.google.com/citations?user=p4zxPP8AAAAJ&hl=zh-CN">Junsong Chen</a>,
               <a href="https://xieenze.github.io/">Enze Xie</a>,
               <strong>Zhongdao Wang</strong>,
               <a href="https://scholar.google.com/citations?user=2p7x6OUAAAAJ&hl=zh-CN">Lanqing Hong</a>,
               <a href="https://scholar.google.com/citations?user=D3nE0agAAAAJ&hl=en">Huchuan Lu</a>,
               <a href="https://scholar.google.com/citations?user=XboZC1AAAAAJ&hl=en">Zhenguo Li</a>,
               <a href="http://luoping.me/">Ping Luo</a>
              <br>
              <em>ICCV</em>, 2023
              
              <br>
              <a href="https://chongjiange.github.io/metabev.html">project page</a> / 
              <a href="https://arxiv.org/abs/2304.09801">arXiv</a> /
              <a href="https://www.youtube.com/watch?v=TiEQpYq77Xo&list=PLB9_L58NpyEWcJhnX-a09CRXp-2kNEojY&index=2">Youtube</a> / 
              <a href="https://github.com/ChongjianGE/MetaBEV">code</a>  <a class="github-button" href="https://github.com/ChongjianGE/MetaBEV" data-icon="octicon-star" data-show-count="true" aria-label="" target="_blank">star</a> 
              
              <br>
              <p></p>
              <p> MetaBEV is a 3D Bird-Eye's View (BEV) perception model that is robust to sensor missing/failure, supporting both single modality mode (camera/lidar) and multi-modal fusion mode with strong performance. </p>
            </td>
          </tr>  
        </tbody></table>
       
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:30%;vertical-align:middle">
                <img src="UniTrack/static/images/framework.png" style="width:auto; height:auto; max-width:100%;">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://zhongdao.github.io/UniTrack">
                <papertitle>Do Different Tracking Tasks Require Different Appearance Models?</papertitle>
              </a>
              <br>
                <strong>Zhongdao Wang</strong>,
              <a href="https://hszhao.github.io">Hengshuang Zhao</a>,
              <a href="https://scholar.google.com/citations?user=aYLh7fsAAAAJ&hl=en&oi=ao">Ya-Li Li</a>,
              <a href="https://scholar.google.com/citations?user=RgzLZZsAAAAJ&hl=en&oi=ao">Shengjin Wang</a>,
              <a href="https://www.robots.ox.ac.uk/~phst/">Philip H.S. Torr</a>,

              <a href="https://www.robots.ox.ac.uk/~luca">Luca Bertinetto</a>
              <br>
              <em>NeurIPS</em>, 2021
              
              <br>
              <a href="https://zhongdao.github.io/UniTrack">project page</a> / 
              <a href="https://arxiv.org/pdf/2107.02156.pdf">arXiv</a> /
              <a href="https://github.com/Zhongdao/UniTrack">code</a>  <a class="github-button" href="https://github.com/Zhongdao/UniTrack" data-icon="octicon-star" data-show-count="true" aria-label="Star Zhongdao/UniTrack on GitHub" target="_blank">star</a>
              
              <br>
              <p></p>
              <p> Object tracking has been fragmented into multiple different experimental setups due to different use cases and benchmarks. We investigate if it is possible to address the various setups with a single, shared apperance model. </p>
            </td>
          </tr>  
        </tbody></table>
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:30%;vertical-align:middle">
                <img src="images/MOT16-03.gif" style="width:auto; height:auto; max-width:100%;">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/1909.12605v2.pdf">
                <papertitle>Towards Real-time Multi-Object Tracking</papertitle>
              </a>
              <br>
              <strong>Zhongdao Wang</strong>,
              <a href="http://www.liangzheng.com.cn/">Liang Zheng</a>,
              Yixuan Liu,
              Yali Li,
              Shengjin Wang
              <br>
              <em>ECCV</em>, 2020
              <br>
              <a href="https://arxiv.org/pdf/1909.12605v2.pdf"> arXiv</a> / 
              <a href="https://github.com/Zhongdao/Towards-Realtime-MOT">code</a>  <a class="github-button" href="https://github.com/Zhongdao/Towards-Realtime-MOT" data-icon="octicon-star" data-show-count="true" aria-label="Star Zhongdao/Towards-Realtime-MOT on GitHub" target="_blank">star</a>
              <br>
              <p></p>
              <p>By incorporating the appearance embedding model into the detector, we introduce JDE, the first open-source real-time multiple object trackor with a running speed of <strong>22 ~ 38</strong> FPS. This speed takes all the steps into account, including detection, appearance embedding extraction and association. Code is released! If you are looking for an easy-to-use and fast pedestrian detector/tracker, JDE is a good option!</p>
            </td>
          </tr>  
        </tbody></table>

        
        <!--
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/dsoftmax.jpg" style="width:auto; height:auto; max-width:100%;">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/1908.01281">
                <papertitle>Softmax Dissection: Towards Understanding Intra-and Inter-class Objective for Embedding Learning</papertitle>
              </a>
              <br>
              Lanqing He*,
              <strong>Zhongdao Wang*</strong>,
              Yali Li, 
              Shengjin Wang (* indicates equal contribution)
              <br>
              <em>AAAI</em>, 2020 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
              <br>
              <a href="https://arxiv.org/pdf/1908.01281">arXiv</a>
              <br>
              <p></p>
              <p>Investigation on intra- and inter-class objectives of the softmax cross-entropy loss function, and a new loss that dissects the two parts for accelerating massive classification.</p>
            </td>
          </tr>
        </tbody></table>

        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/gcn.jpg" style="width:auto; height:auto; max-width:100%;">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Wang_Linkage_Based_Face_Clustering_via_Graph_Convolution_Network_CVPR_2019_paper.pdf">
                <papertitle>Linkage based face clustering via graph convolution network</papertitle>
              </a>
              <br>
              <strong>Zhongdao Wang</strong>,
              <a href="http://www.liangzheng.com.cn/">Liang Zheng</a>,
              Yali Li,
              Shengjin Wang 
              <br>
              <em>CVPR</em>, 2019 
              <br>
              <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Wang_Linkage_Based_Face_Clustering_via_Graph_Convolution_Network_CVPR_2019_paper.pdf"> paper</a> / 
              <a href="https://github.com/Zhongdao/gcn_clustering">code</a>  <a class="github-button" href="https://github.com/Zhongdao/gcn_clustering" data-icon="octicon-star" data-show-count="true" aria-label="Star Zhongdao/gcn_clustering on GitHub" target="_blank">star</a>
              <br>
              <p></p>
              <p>A supervised solution to the face clustering task using graph convolutional networks.</p>
            </td>
          </tr>
        </tbody></table>
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/iccv17.png" style="width:auto; height:auto; max-width:100%;">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Wang_Orientation_Invariant_Feature_ICCV_2017_paper.pdf">
                <papertitle>Orientation invariant feature embedding and spatial temporal regularization for vehicle re-identification</papertitle>
              </a>
              <br>
              <strong>Zhongdao Wang*</strong>,
              <a href="http://hal9000.space/">Luming Tang*</a>,
              <a href="https://xh-liu.github.io/">Xihui Liu</a>,
              <a href="https://scholar.google.com/citations?user=J3kgC1QAAAAJ&hl=en-US">Zhuliang Yao</a>,
              <a href="https://scholar.google.com/citations?user=afbbNmwAAAAJ&hl=en-US">Shuai Yi</a>,
              <a href="https://scholar.google.com/citations?user=VU5ObUwAAAAJ&hl=en-US&oi=ao">Jing shao</a>,
              <a href="https://scholar.google.com/citations?user=rEYarG0AAAAJ&hl=en-US&oi=ao">Junjie Yan</a>,
              <a href="https://scholar.google.com/citations?hl=en-US&user=RgzLZZsAAAAJ">Shengjin Wang</a>,
              <a href="http://www.ee.cuhk.edu.hk/~hsli/">Hongsheng Li</a>,
              <a href="https://scholar.google.com/citations?user=-B5JgjsAAAAJ&hl=en-US">Xiaogang Wang</a> 
              (* indicates equal contribution)
              <br>
              <em>ICCV</em>, 2017
              <br>
              <a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Wang_Orientation_Invariant_Feature_ICCV_2017_paper.pdf"> paper </a> / 
              <a href="https://github.com/Zhongdao/VehicleReIDKeyPointData">Key point annotation for Veri-776 dataset</a> <a class="github-button" href="https://github.com/Zhongdao/VehicleReIDKeyPointData" data-icon="octicon-star" data-show-count="true" aria-label="Star Zhongdao/VehicleReIDKeyPointData on GitHub" target="_blank">star</a>
              <br>
              <p></p>
              <p> An orientation-invariant solution to the vehicle re-identification problem. </p>
            </td>
          </tr>
        </tbody>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/MTMCT.png" style="width:auto; height:auto; max-width:100%;">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/document/9646485">
                <papertitle>Adapting Affinity in Association Improves Multi-Target Multi-Camera Tracking</papertitle>
              </a>
              <br>
              <a href="https://hou-yz.github.io/">Yunzhong Hou</a>,
              <strong>Zhongdao Wang</strong>,
              <a href="https://scholar.google.com/citations?hl=en-US&user=RgzLZZsAAAAJ">Shengjin Wang</a>,
              <a href="http://www.liangzheng.com.cn/">Liang Zheng</a>
              <br>
              <em>IEEE Transactions on Image Processing (TIP)</em>, 2021
              <br>
              <p></p>
              <p> Model the multi-target multi-camera tracking (MTMCT) problem from a perspective of local matching. </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/evt.jpg" style="width:auto; height:auto; max-width:100%;">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/abstract/document/8977386">
                <papertitle>Node-Adaptive Multi-Graph Fusion Using Extreme Value Theory</papertitle>
              </a>
              <br>
              Jingwei Zhang,
              <strong>Zhongdao Wang</strong>,
              Yali Li,
              <a href="https://scholar.google.com/citations?hl=en-US&user=RgzLZZsAAAAJ">Shengjin Wang</a>,
              <br>
              <em>IEEE Singnal Processing Letters (SPL)</em>, 2020
              <br>
              <p></p>
              <p> Multi-graph fusion using extreme value theory for multi-view clustering. </p>
            </td>
          </tr>
        </tbody></table>
        -->

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
          <h3 id="representation"> Representation learning / re-identification </h3>
            </td>
          </tr>
      </tbody></table>
        
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:30%;vertical-align:middle">
                <img src="images/ISR.png" style="width:auto; height:auto; max-width:100%;">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="7">
                <papertitle>[CycAs v3] Identity-Seeking Self-Supervised Representation Learning for Generalizable Person Re-identification</papertitle>
              </a>

              <br>
              Zhaopeng Dou,
              <strong>Zhongdao Wang</strong>,
              Yali Li,
              Shengjin Wang
              <br>
              <em>ICCV</em>, 2023
              <p></p>
              <p> A new self-supervised learning objective further improves upon CycAs v2, refreshing the zero-shot transfer performance on Market-1501 to 87.1% rank-1 accuracy, already on par with supervised models trained with in-domain data.</p>
            </td>
          </tr>  
        </tbody></table>
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:30%;vertical-align:middle">
                <img src="images/CycAs.png" style="width:auto; height:auto; max-width:100%;">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2007.07577">
                <papertitle>[CycAs v1] CycAs: Self-supervised Cycle Association for Learning Re-identifiable Descriptions</papertitle>
              </a>
              <br>
              <a href="https://arxiv.org/abs/2211.03663">
                <papertitle>[CycAs v2] Generalizable Re-Identification from Videos with Cycle Association</papertitle>
              </a>
              <br>
              <strong>Zhongdao Wang</strong>,
              Jingwei Zhang,
              <a href="http://www.liangzheng.com.cn/">Liang Zheng</a>,
              Yixuan Liu,
              Yifan Sun,
              Yali Li,
              Shengjin Wang
              <br>
              <em>[v1] ECCV</em>, 2020; <em>[v2] arXiv</em>, 2021 
              <p> CycAs is the fisrt self-supervised method for learning re-identifiable features. We use videos as input, the supervision signal is cycle consistency emerging from instance association in a forward-backward cycle. In CycAs v1, we train and show promising results on (mostly small) canonical datasets; In CycAs v2, we collect a large amount of video data and train a much more generalizable model with improved building blocks. The zero-shot transfer ablility is suprisingly good, with a rank-1 accuracy of 82.2% on Market-1501.</p>
            </td>
          </tr>  
        </tbody></table>
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:30%;vertical-align:middle">
                <img src="images/MEC.png" style="width:auto; height:auto; max-width:100%;">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/dc709714c52b35f2f34aca2a92b06bc8-Paper-Conference.pdf">
                <papertitle>Self-Supervised Learning via Maximum Entropy Coding</papertitle>
              </a>
              <br>
              Xin Liu,
              <strong>Zhongdao Wang</strong>,
              Yali Li,
              Shengjin Wang

              <br>
              <em>NeurIPS</em>, 2022 &nbsp <font color="red">(Spotlight)</font>
              <p></p>
              <p>Self-supervised learning via Maximum Entropy Coding improves generalization ability of the learned representation on a variaty of down-stream tasks.</p>
            </td>
          </tr>
        </tbody></table>
        
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:30%;vertical-align:middle">
                <img src="images/circleloss.png" style="width:auto; height:auto; max-width:100%;">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2002.10857.pdf">
                <papertitle>Circle Loss: A Unified Perspective of Pair Similarity Optimization</papertitle>
              </a>
              <br>
              <a href="https://scholar.google.com/citations?user=uUZEL7UAAAAJ&hl=en-US">Yifan Sun</a>,
              <a href="https://github.com/wandering007">Changmao Cheng</a>,
              Yuhan Zhang, 
              <a href="https://github.com/wandering007">Chi Zhang</a>,
              <a href="http://www.liangzheng.com.cn/">Liang Zheng</a>,
              <strong>Zhongdao Wang</strong>,
              <a href="https://yichenwei.github.io/">Yichen Wei</a>,

              <br>
              <em>CVPR</em>, 2020 &nbsp <font color="red">(Oral)</font>
              <br>
              <a href="https://arxiv.org/pdf/2002.10857.pdf"> arXiv</a>
              <br>
              <p></p>
              <p>A unified perspective for proxy-based and pair-based metric learning.</p>
            </td>
          </tr>
        </tbody></table>

      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:center;font-size:small;">
                Design and source code modified based on  <a href="https://jonbarron.info/">Jon Barron's website</a>.
              </p>
            </td>
          </tr>
        </tbody></table>
      
      </td>
    </tr>
  </table>
</body>

</html>
