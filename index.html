<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <!-- Hi, Jon Here. Please DELETE the two <script> tags below if you use this HTML, otherwise my analytics will track your page -->
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <!--
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-7580334-2"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-7580334-2');
  </script>
  -->
  <title>Zhongdao Wang</title>
  
  <meta name="author" content="Zhongdao Wang">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<script async defer src="https://buttons.github.io/buttons.js"></script>
<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Zhongdao Wang</name>
              </p>
              <p>I am currently a researcher at Noah's Ark Lab, Huawei. Before joining Huawei, I recieved my Ph.D. degree from the Department of Electronic Engineering, Tsinghua Univerisity, and my bechalor's degree from the Department of Physics, Tsinghua University. During my Ph.D. time, I have been fortunate to work closely with Prof. <a href="http://www.liangzheng.com.cn/">Liang Zheng</a>,
              Dr. <a herf="https://yifansun-reid.github.io/">Yifan Sun</a>, <a href="https://www.robots.ox.ac.uk/~luca/">Dr. Luca Bertinetto</a>, and <a href="https://hszhao.github.io/">Prof. Hengshuang Zhao</a>
              </p>
              <p style="text-align:center">
                <a href="mailto:wangzhongdao@huawei.com">Email</a> &nbsp/&nbsp
                <a href="data/cv_zhongdaowang.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=VkKYpHIAAAAJ&hl=en-US">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/Zhongdao">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/me.jpeg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/me.jpeg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p> 
              Currently, I am interested in perception algorithms for autonomous driving, including but not limited to 3D object detection/tracking, network archtecture/learning algorithm/pre-training for multi-modal fusion, and 4D Auto-labeling. 
              </p>
              <p>
              <font color="red">I have several openings for self-motivated reserach interns. If you are interested in working with me on the abovementioned challenging topics, please feel free to drop me an Email!</font> 
              </p>
              </p>
              My previous research interests were task-oriented self-supervised learning, espicially applications on multiple object tracking (MOT) and re-identification (re-ID). Previously, I also did research on metric learning and its applications on face recognition, person/vehicle re-identification.
              Representative works include <strong><a href="https://github.com/Zhongdao/Towards-Realtime-MOT">JDE</a></strong> and <strong><a href="https://zhongdao.github.io/UniTrack">UniTrack</a></strong>. 
              <p>
            </td>
          </tr>
        </tbody></table>
        <!-- 
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Publications</heading>
              <br>
              <h3>Preprint papers</h3>
            </td>
          </tr>
        </tbody></table>
        -->
        
        
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Publications</heading>
            </td>
          </tr>
       
      </tbody></table>
       
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/miex.png" style="width:auto; height:auto; max-width:100%;">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2112.01730">
                <papertitle>How to Synthesize a Large-Scale and Trainable Micro-Expression Dataset?</papertitle>
              </a>
              <br>
              Yuchi Liu,
                <strong>Zhongdao Wang</strong>,
              <a href="http://users.cecs.anu.edu.au/~Tom.Gedeon/">Tom Gedeon</a>,
              <a href="http://www.liangzheng.com.cn/">Liang Zheng</a>,
              <br>
              <em>ECCV</em>, 2022
              
              <br>
              <a href="https://arxiv.org/abs/2112.01730">arXiv</a> /
              <a href="https://github.com/liuyvchi/MiE-X">code</a>  <a class="github-button" href="https://github.com/liuyvchi/MiE-X" data-icon="octicon-star" data-show-count="true" aria-label="Star Zhongdao/UniTrack on GitHub" target="_blank">star</a>
              
              <br>
              <p></p>
              <p>  </p>
            </td>
          </tr>  
        </tbody></table>

       
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/prob_reid.png" style="width:auto; height:auto; max-width:100%;">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org">
                <papertitle>Reliability-Aware Prediction via Uncertainty Learning for Person Image Retrieval</papertitle>
              </a>
              <br>
              Zhaopeng Dou,
              <strong>Zhongdao Wang</strong>,
              Weihua Chen,
              <a href="https://scholar.google.com/citations?user=aYLh7fsAAAAJ&hl=en&oi=ao">Ya-Li Li</a>,
              <a href="https://scholar.google.com/citations?user=RgzLZZsAAAAJ&hl=en&oi=ao">Shengjin Wang</a>,
              <br>
              <em>ECCV</em>, 2022
              
              <br>
              <p></p>
              <p> Joint modeling of data and model uncertainty improves robustness and reliability of Re-ID representations.</p>
            </td>
          </tr>  
        </tbody></table>
       
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="UniTrack/static/images/framework.png" style="width:auto; height:auto; max-width:100%;">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://zhongdao.github.io/UniTrack">
                <papertitle>Do Different Tracking Tasks Require Different Appearance Models?</papertitle>
              </a>
              <br>
                <strong>Zhongdao Wang</strong>,
              <a href="https://hszhao.github.io">Hengshuang Zhao</a>,
              <a href="https://scholar.google.com/citations?user=aYLh7fsAAAAJ&hl=en&oi=ao">Ya-Li Li</a>,
              <a href="https://scholar.google.com/citations?user=RgzLZZsAAAAJ&hl=en&oi=ao">Shengjin Wang</a>,
              <a href="https://www.robots.ox.ac.uk/~phst/">Philip H.S. Torr</a>,

              <a href="https://www.robots.ox.ac.uk/~luca">Luca Bertinetto</a>
              <br>
              <em>NeurIPS</em>, 2021
              
              <br>
              <a href="https://zhongdao.github.io/UniTrack">project page</a> / 
              <a href="https://arxiv.org/pdf/2107.02156.pdf">arXiv</a> /
              <a href="https://github.com/Zhongdao/UniTrack">code</a>  <a class="github-button" href="https://github.com/Zhongdao/UniTrack" data-icon="octicon-star" data-show-count="true" aria-label="Star Zhongdao/UniTrack on GitHub" target="_blank">star</a>
              
              <br>
              <p></p>
              <p> Object tracking has been fragmented into multiple different experimental setups due to different use cases and benchmarks. We investigate if it is possible to address the various setups with a single, shared apperance model. </p>
            </td>
          </tr>  
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/CycAs.png" style="width:auto; height:auto; max-width:100%;">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2007.07577.pdf">
                <papertitle>CycAs: Self-supervised Cycle Association for Learning Re-identifiable Descriptions</papertitle>
              </a>
              <br>
              <strong>Zhongdao Wang</strong>,
              Jingwei Zhang,
              <a href="http://www.liangzheng.com.cn/">Liang Zheng</a>,
              Yixuan Liu,
              Yifan Sun,
              Yali Li,
              Shengjin Wang
              <br>
              <em>ECCV</em>, 2020
              
              <br>  
              <a href="https://arxiv.org/pdf/2007.07577.pdf">arXiv</a>
              <br>
              <p></p>
              <p> Learning re-identifiable features by self-supervised learning. The supervision signal is cycle consistency emerging from instance association in a forward-then-backward cycle. </p>
            </td>
          </tr>  
        </tbody></table>
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/MOT16-03.gif" style="width:auto; height:auto; max-width:100%;">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/1909.12605v2.pdf">
                <papertitle>Towards Real-time Multi-Object Tracking</papertitle>
              </a>
              <br>
              <strong>Zhongdao Wang</strong>,
              <a href="http://www.liangzheng.com.cn/">Liang Zheng</a>,
              Yixuan Liu,
              Yali Li,
              Shengjin Wang
              <br>
              <em>ECCV</em>, 2020
              <br>
              <a href="https://arxiv.org/pdf/1909.12605v2.pdf"> arXiv</a> / 
              <a href="https://github.com/Zhongdao/Towards-Realtime-MOT">code</a>  <a class="github-button" href="https://github.com/Zhongdao/Towards-Realtime-MOT" data-icon="octicon-star" data-show-count="true" aria-label="Star Zhongdao/Towards-Realtime-MOT on GitHub" target="_blank">star</a>
              <br>
              <p></p>
              <p>By incorporating the appearance embedding model into the detector, we introduce JDE, the first open-source real-time multiple object trackor with a running speed of <strong>22 ~ 38</strong> FPS. This speed takes all the steps into account, including detection, appearance embedding extraction and association. Code is released! If you are looking for an easy-to-use and fast pedestrian detector/tracker, JDE is a good option!</p>
            </td>
          </tr>  
        </tbody></table>
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/circleloss.png" style="width:auto; height:auto; max-width:100%;">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2002.10857.pdf">
                <papertitle>Circle Loss: A Unified Perspective of Pair Similarity Optimization</papertitle>
              </a>
              <br>
              <a href="https://scholar.google.com/citations?user=uUZEL7UAAAAJ&hl=en-US">Yifan Sun</a>,
              <a href="https://github.com/wandering007">Changmao Cheng</a>,
              Yuhan Zhang, 
              <a href="https://github.com/wandering007">Chi Zhang</a>,
              <a href="http://www.liangzheng.com.cn/">Liang Zheng</a>,
              <strong>Zhongdao Wang</strong>,
              <a href="https://yichenwei.github.io/">Yichen Wei</a>,

              <br>
              <em>CVPR</em>, 2020 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
              <br>
              <a href="https://arxiv.org/pdf/2002.10857.pdf"> arXiv</a>
              <br>
              <p></p>
              <p>A unified perspective for proxy-based and pair-based metric learning.</p>
            </td>
          </tr>
        </tbody></table>
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/dsoftmax.jpg" style="width:auto; height:auto; max-width:100%;">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/1908.01281">
                <papertitle>Softmax Dissection: Towards Understanding Intra-and Inter-class Objective for Embedding Learning</papertitle>
              </a>
              <br>
              Lanqing He*,
              <strong>Zhongdao Wang*</strong>,
              Yali Li, 
              Shengjin Wang (* indicates equal contribution)
              <br>
              <em>AAAI</em>, 2020 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
              <br>
              <a href="https://arxiv.org/pdf/1908.01281">arXiv</a>
              <br>
              <p></p>
              <p>Investigation on intra- and inter-class objectives of the softmax cross-entropy loss function, and a new loss that dissects the two parts for accelerating massive classification.</p>
            </td>
          </tr>
        </tbody></table>

        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/gcn.jpg" style="width:auto; height:auto; max-width:100%;">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Wang_Linkage_Based_Face_Clustering_via_Graph_Convolution_Network_CVPR_2019_paper.pdf">
                <papertitle>Linkage based face clustering via graph convolution network</papertitle>
              </a>
              <br>
              <strong>Zhongdao Wang</strong>,
              <a href="http://www.liangzheng.com.cn/">Liang Zheng</a>,
              Yali Li,
              Shengjin Wang 
              <br>
              <em>CVPR</em>, 2019 
              <br>
              <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Wang_Linkage_Based_Face_Clustering_via_Graph_Convolution_Network_CVPR_2019_paper.pdf"> paper</a> / 
              <a href="https://github.com/Zhongdao/gcn_clustering">code</a>  <a class="github-button" href="https://github.com/Zhongdao/gcn_clustering" data-icon="octicon-star" data-show-count="true" aria-label="Star Zhongdao/gcn_clustering on GitHub" target="_blank">star</a>
              <br>
              <p></p>
              <p>A supervised solution to the face clustering task using graph convolutional networks.</p>
            </td>
          </tr>
        </tbody></table>
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/iccv17.png" style="width:auto; height:auto; max-width:100%;">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Wang_Orientation_Invariant_Feature_ICCV_2017_paper.pdf">
                <papertitle>Orientation invariant feature embedding and spatial temporal regularization for vehicle re-identification</papertitle>
              </a>
              <br>
              <strong>Zhongdao Wang*</strong>,
              <a href="http://hal9000.space/">Luming Tang*</a>,
              <a href="https://xh-liu.github.io/">Xihui Liu</a>,
              <a href="https://scholar.google.com/citations?user=J3kgC1QAAAAJ&hl=en-US">Zhuliang Yao</a>,
              <a href="https://scholar.google.com/citations?user=afbbNmwAAAAJ&hl=en-US">Shuai Yi</a>,
              <a href="https://scholar.google.com/citations?user=VU5ObUwAAAAJ&hl=en-US&oi=ao">Jing shao</a>,
              <a href="https://scholar.google.com/citations?user=rEYarG0AAAAJ&hl=en-US&oi=ao">Junjie Yan</a>,
              <a href="https://scholar.google.com/citations?hl=en-US&user=RgzLZZsAAAAJ">Shengjin Wang</a>,
              <a href="http://www.ee.cuhk.edu.hk/~hsli/">Hongsheng Li</a>,
              <a href="https://scholar.google.com/citations?user=-B5JgjsAAAAJ&hl=en-US">Xiaogang Wang</a> 
              (* indicates equal contribution)
              <br>
              <em>ICCV</em>, 2017
              <br>
              <a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Wang_Orientation_Invariant_Feature_ICCV_2017_paper.pdf"> paper </a> / 
              <a href="https://github.com/Zhongdao/VehicleReIDKeyPointData">Key point annotation for Veri-776 dataset</a> <a class="github-button" href="https://github.com/Zhongdao/VehicleReIDKeyPointData" data-icon="octicon-star" data-show-count="true" aria-label="Star Zhongdao/VehicleReIDKeyPointData on GitHub" target="_blank">star</a>
              <br>
              <p></p>
              <p> An orientation-invariant solution to the vehicle re-identification problem. </p>
            </td>
          </tr>
        </tbody>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/MTMCT.png" style="width:auto; height:auto; max-width:100%;">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/document/9646485">
                <papertitle>Adapting Affinity in Association Improves Multi-Target Multi-Camera Tracking</papertitle>
              </a>
              <br>
              <a href="https://hou-yz.github.io/">Yunzhong Hou</a>,
              <strong>Zhongdao Wang</strong>,
              <a href="https://scholar.google.com/citations?hl=en-US&user=RgzLZZsAAAAJ">Shengjin Wang</a>,
              <a href="http://www.liangzheng.com.cn/">Liang Zheng</a>
              <br>
              <em>IEEE Transactions on Image Processing (TIP)</em>, 2021
              <br>
              <p></p>
              <p> Model the multi-target multi-camera tracking (MTMCT) problem from a perspective of local matching. </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/evt.jpg" style="width:auto; height:auto; max-width:100%;">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/abstract/document/8977386">
                <papertitle>Node-Adaptive Multi-Graph Fusion Using Extreme Value Theory</papertitle>
              </a>
              <br>
              Jingwei Zhang,
              <strong>Zhongdao Wang</strong>,
              Yali Li,
              <a href="https://scholar.google.com/citations?hl=en-US&user=RgzLZZsAAAAJ">Shengjin Wang</a>,
              <br>
              <em>IEEE Singnal Processing Letters (SPL)</em>, 2020
              <br>
              <p></p>
              <p> Multi-graph fusion using extreme value theory for multi-view clustering. </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:center;font-size:small;">
                Design and source code modified based on  <a href="https://jonbarron.info/">Jon Barron's website</a>.
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
